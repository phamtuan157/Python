# -*- coding: utf-8 -*-
"""Assignment3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1BYPm89tIjum-L27oGxkhmwWNFB9VpnFD
"""



# Commented out IPython magic to ensure Python compatibility.
from google.colab import drive
drive.mount('/gdrive')
# %cd /gdrive

pip install vecstack

from vecstack import stacking
import pandas as pd
from sklearn.preprocessing import label_binarize
import numpy as np
from sklearn.metrics import accuracy_score
from sklearn.neural_network import MLPClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn import svm
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import RandomizedSearchCV
from sklearn.model_selection import cross_val_score
from sklearn.metrics import confusion_matrix,classification_report
from sklearn.ensemble import GradientBoostingClassifier
from imblearn.over_sampling import SMOTE
from sklearn.svm import LinearSVC
from collections import Counter #for Smote

import warnings
warnings.filterwarnings("ignore")



#to upload our datasets from our working directory we need to moun our drive contents to the collab environment

trainfile = r'/gdrive/My Drive/CIS508/RevisedHomesiteTrain.csv'
traindata = pd.read_csv(trainfile)

   

testfile = r'/gdrive/My Drive/CIS508/RevisedHomesiteTest.csv'
testdata = pd.read_csv(testfile)



print(traindata.shape)
print(traindata.head()) 


print(testdata.shape)
print(testdata.head())

print(traindata.columns)
print("---------------")
print(testdata.columns)

print(traindata.info)

print(testdata.info)

#Copy Train data excluding target
traindata_Copy = traindata.iloc[:, :-1].copy()
testdata_Copy = testdata.iloc[:, :-1].copy()

#Separate Train data and test data
x_train = traindata_Copy
x_test = testdata_Copy

#Select just Target Column
y_train = traindata.iloc[:, -1]
#y_test = testdata.iloc[:, :-1]

print(x_train.shape)
print(x_test.head()) 

print(y_train.shape)
print(y_train.head())

#Dt
clf = DecisionTreeClassifier()
clf.fit(x_train, y_train)
clf_predict=clf.predict(x_test)
y_pred= clf.predict (x_test)

y_pred =  pd.DataFrame({'QuoteNumber':testdata['QuoteNumber'],'QuoteConversion_Flag':y_pred})
y_pred.to_csv("/gdrive/My Drive/CIS508/trainresults.csv",index = False)
print(y_pred['QuoteConversion_Flag'].value_counts())
y_pred.head()





parameters = {'min_samples_split' : range(20,100,10), 'max_depth':range(1,20,2)}
clf_random = RandomizedSearchCV(clf,parameters,n_iter=10)
clf_random.fit(x_train,y_train)
grid_parm=clf_random.best_params_
print(grid_parm)

clf = DecisionTreeClassifier(**grid_parm)
clf.fit(x_train,y_train)
clf_predict = clf.predict(x_test)
print("Hypertuned decision tree classifier")  # obtain accuracy, confusion matrix, classification report

clf_cv_score = cross_val_score(clf,x_train,y_train,cv = 15, scoring = 'roc_auc')
print("All AUC Score")
print(clf_cv_score)
print('\n')

print('Mean Auc SCore')
print('Mean Aus score - DT',clf_cv_score.mean())

#random forest
rfc = RandomForestClassifier()
rfc.fit(x_train, y_train)
rfc_predict=rfc.predict(x_test)
print("Random Forest")




rfc_predict =  pd.DataFrame({'QuoteNumber':testdata['QuoteNumber'],'QuoteConversion_Flag':rfc_predict})
rfc_predict.to_csv('/gdrive/My Drive/CIS508/RFResult.csv', index=False)

#hyperparameter tuning for random forest
parameters = {'n_estimators' : range(50,100,20),'min_samples_split': range(10,100,20),'max_depth':range(1,20,20)}
rfc_random = RandomizedSearchCV(rfc,parameters, n_iter=15)
rfc_random.fit(x_train,y_train)
grid_parm_rfc = rfc_random.best_params_
print(grid_parm_rfc)

#using better parameters
rfc = RandomForestClassifier(**grid_parm_rfc)
rfc.fit(x_train,y_train)
rfc_predict = rfc.predict(x_test)
print('Hypertuned random Forest')

#run cross-validation on it
rfc_cv_score = cross_val_score(rfc,x_train,y_train,cv=15,scoring="roc_auc")
print("All AUC scores")
print(rfc_cv_score)
print('\n')
print("mean Auc score")
print('Mean Auc score - random forest:', rfc_cv_score.mean())

#Gradient Boosting
search_grid={'n_estimators':[5,10,15,20,40], 'learning_rate':[0.05,.1]}
xyz = GradientBoostingClassifier()
xyz.fit(x_train,y_train)
xyz_predict = xyz.predict(x_test)
print("Gradient Booster")

#randomized search for hyperparameter tuning
xyz_random = RandomizedSearchCV(xyz, search_grid, n_iter=15)
xyz_random.fit(x_train,y_train)
grid_parm_xyz = xyz.predict(x_test)
print("GRadient Boosting with best params")

# cross-validation on best params
xyz_cv_score = cross_val_score(xyz,x_train,y_train, cv=10, scoring="roc_auc")
print("All AUC scores")
print(xyz_cv_score)
print('\n')
print("mean Auc score")
print('Mean Auc score - random forest:', xyz_cv_score.mean())

#Contruct Multilayer Perception using the best params

mlp = MLPClassifier()
mlp.fit(x_train,y_train)
mlp_predict = mlp.predict(x_test)
print("MLP")

#Construct Support Vector Machines using the best parameters
#Default mode
svm = svm.LinearSVC()
svm.fit(x_train, y_train)
svm_predict_train = svm.predict(x_test)
svm_predict_train = pd.DataFrame({'QuoteNumber':testdata['QuoteNumber'],'QuoteConversion_Flag':svm_predict_train})
svm_predict_train.to_csv('/gdrive/My Drive/CIS508/SVMResult.csv', index=False)

#K-Nearest Neighbors
knn = KNeighborsClassifier()
knn.fit(x_train,y_train)
knn_predict_train = knn.predict(x_test)

#Hyperparameter tuning for KN-Nearest Neighbors
parameters={ 'n_neighbors': range(1,20,2)}
knn_random = RandomizedSearchCV(knn,parameters,n_iter=15)
knn_random.fit(x_train, y_train)
grid_parm_knn=knn_random.best_params_
print(grid_parm_knn)

#contruct KN-Nearest Neighbors using the best parameters
knn= KNeighborsClassifier(**grid_parm_knn)
knn.fit(x_train,y_train)

#run cross-validation on best parameters, get auc score
knn_cv_score = cross_val_score(knn, x_train, y_train, cv=10, scoring="roc_auc")
print("=== All AUC Scores ===")
print(knn_cv_score)
print('\n')
print("=== Mean AUC Score ===")
print("Mean AUC Score - KN-Nearest Neighbors: ",knn_cv_score.mean())

#SMOTE------------------------------
print("___________________________\nSMOTE\n")
print('Original dataset shape %s' % Counter(y_train))
sm= SMOTE(sampling_strategy = 'float', ratio = 0.6)
x_res,y_res = sm.fit_resample(x_train,y_train)
print("Resampled dataset shape %s" % Counter(y_res))

#STACKING MODELS =====================================================================
print("___________________________________________________________________________________________\nEnsemble Methods Predictions using GradientBoosting, RandomForest and Decision Tree Classifier\n")

models = [DecisionTreeClassifier(min_samples_split=70, max_depth=5), RandomForestClassifier(n_estimators=90, min_samples_split= 30, max_depth= 17), MLPClassifier(), LinearSVC(), KNeighborsClassifier() ]
      
S_Train, S_Test = stacking(models,                   
                           x_res, y_res, x_test,   
                           regression=False, 
     
                           mode='oof_pred_bag', 
       
                           needs_proba=False,
         
                           save_dir=None, 
            
                           metric=accuracy_score, 
    
                           n_folds=4, 
                 
                           stratified=True,
            
                           shuffle=True,  
            
                           random_state=0,    
         
                           verbose=2)

#Stacking - construct a gradient Boosting model ----

search_grid = {'n_estimators': [5,10,15,20,40], 'learning_rate':[0.01,.1]}
final = GradientBoostingClassifier()
final.fit(S_Train,y_res)
y_pred=final.predict(S_Test)
y_pred_df = pd.DataFrame({'QuoteNumber':testdata['QuoteNumber'], 'QuoteConversion_Flag':y_pred})
y_pred_df.to_csv('/gdrive/My Drive/CIS508/Stacking.csv', index=False)
print("Gradient Booster")

#Randomized Search for hyperparameter tuning
final_random = RandomizedSearchCV(final,search_grid,n_iter=15)
final_random.fit(S_Train,y_res)
grid_parm_xyz = final_random.best_params_
print(grid_parm_xyz)
print("Done")



print(y_pred_df.head())